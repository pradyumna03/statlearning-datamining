{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaSGd67jFWQ0"
   },
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNoTmr_1LRfY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZmJt7xRK2xp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'/content/drive/MyDrive/prads_assignment/data_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9fO6nHLLAxH"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(r'/content/drive/MyDrive/prads_assignment/data_testUNLABELED.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "266it-uuNEiy"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWTGqgSMFZpd"
   },
   "source": [
    "# Defining Error Rate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y792-6YyOjDE"
   },
   "outputs": [],
   "source": [
    "def balanced_error_rate(y_pred, y_actual):\n",
    "  unique, counts = numpy.unique(y_sample_test, return_counts=True)\n",
    "  totalActualValues = dict(zip(unique, counts))\n",
    "  predictedValues = {\n",
    "      '0':{'0':0, '1': 0, '2':0},\n",
    "      '1':{'0':0, '1': 0, '2':0},\n",
    "      '2':{'0':0, '1': 0, '2':0}\n",
    "  }\n",
    "  for i in range(len(y_pred)):\n",
    "    predictedValues[str(y_actual[i])][str(y_pred[i])]+=1\n",
    "  errorSumForRates = {\n",
    "      '0':0,\n",
    "      '1':0,\n",
    "      '2':0\n",
    "  }\n",
    "  for key in predictedValues.keys():\n",
    "    for keyLevel2 in predictedValues[key].keys():\n",
    "      if (keyLevel2!=key):\n",
    "        errorSumForRates[str(key)]+=predictedValues[key][keyLevel2]\n",
    "  sumBalancedError = 0\n",
    "  errorRateOnEachClass = {\n",
    "      '0':0,\n",
    "      '1':0,\n",
    "      '2':0\n",
    "  }\n",
    "  for key in errorSumForRates.keys():\n",
    "    sumBalancedError += (errorSumForRates[str(key)]/totalActualValues[int(key)])\n",
    "    errorRateOnEachClass[str(key)] = (errorSumForRates[str(key)]/totalActualValues[int(key)])\n",
    "  return sumBalancedError/len(errorSumForRates.keys()), errorRateOnEachClass, predictedValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5ws99bQ64xZ"
   },
   "source": [
    "# Baseline Method\n",
    "## Predicting values for the non-preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ1r8n2wFe2C"
   },
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pE6xfC569KI"
   },
   "outputs": [],
   "source": [
    "X = df.drop('y', axis = 1)\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_sample_test, y_train, y_sample_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF8RISMjFkRr"
   },
   "source": [
    "### Fitting training data to a basic Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UCEuang7teC",
    "outputId": "c2fc7c7c-add9-42b7-dffb-5c7d7841aa16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators= 10)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gga9hlkPFprP"
   },
   "source": [
    "### Predicting the result for the sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NHYzfyc69sO"
   },
   "outputs": [],
   "source": [
    "y_pred_sample_test = model.predict(X_sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZnVskLMF_Cu"
   },
   "source": [
    "### Finding the balanced error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7Sfr9pW69sO"
   },
   "outputs": [],
   "source": [
    "total_balanced_error_rate, errorRateOnEachClass, predictedValues = balanced_error_rate(y_pred_sample_test.tolist(),y_sample_test.values.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzGb-ueq69sO",
    "outputId": "1bba7f99-ebda-49b6-e260-8cb732ceef29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual          Predicted 0     Predicted 1     Predicted 2     Error Rate     \n",
      "\n",
      "Class 0         122             6               99              0.46255506607929514\n",
      "Class 1         23              57              37              0.5128205128205128\n",
      "Class 2         60              13              209             0.25886524822695034\n",
      "\n",
      "\n",
      "Overall Error Rate : 0.4114136090422528\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n{:<15} {:<15} {:<15} {:<15} {:<15}\\n\".format('Actual', 'Predicted 0', 'Predicted 1', 'Predicted 2', 'Error Rate'))\n",
    " \n",
    "for keyUp, valueUP in predictedValues.items():\n",
    "  listOutput = []\n",
    "  for key, value in predictedValues[keyUp].items():\n",
    "    listOutput.append(value)\n",
    "  pred0, pred1, pred2 = listOutput[0], listOutput[1],listOutput[2]\n",
    "  print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('Class '+str(keyUp), pred0, pred1, pred2,errorRateOnEachClass[str(keyUp)]))\n",
    "\n",
    "print(\"\\n\\nBalanced Error Rate : \" + str(total_balanced_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF3DRPWWGFSd"
   },
   "source": [
    "### Classification report for the rest of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8RnvM_-69sP",
    "outputId": "95c2dfcd-8a63-4a90-cec7-0375e3d4a87b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.54      0.60      0.56       205\n",
      "     class 1       0.49      0.75      0.59        76\n",
      "     class 2       0.74      0.61      0.67       345\n",
      "\n",
      "    accuracy                           0.62       626\n",
      "   macro avg       0.59      0.65      0.61       626\n",
      "weighted avg       0.64      0.62      0.62       626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_pred_sample_test.tolist(),y_sample_test.values.tolist(), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcQLoMdyGRF7"
   },
   "source": [
    "# Random Forest Method\n",
    "## Predicting values for the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLnW6lmqaCea"
   },
   "source": [
    "###Normalizing columns to same range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYiBZoPWU3pT"
   },
   "outputs": [],
   "source": [
    "df_min_max_scaled = df.copy()\n",
    "for key in df.keys():\n",
    "  if key!= 'y':\n",
    "    column = key\n",
    "    df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())    \n",
    "\n",
    "df_min_max_scaled = df_min_max_scaled.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fJUGq_CaG3a"
   },
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5K7k5TaMbWU4"
   },
   "outputs": [],
   "source": [
    "X = df_min_max_scaled.drop('y', axis = 1)\n",
    "y = df_min_max_scaled['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEl8KPorOCyV"
   },
   "outputs": [],
   "source": [
    "X_train, X_sample_test, y_train, y_sample_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFyNQz4pGcu_"
   },
   "source": [
    "### Comparing the ratios of labels in the split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc7Bm5wftmis",
    "outputId": "4f24d270-c927-4025-9c24-21f08b70c645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 829, '1': 402, '2': 1272}\n",
      "0 \t 0.331202556931682\n",
      "1 \t 0.16060727127447064\n",
      "2 \t 0.5081901717938474\n",
      "{'0': 227, '1': 117, '2': 282}\n",
      "0 \t 0.36261980830670926\n",
      "1 \t 0.1869009584664537\n",
      "2 \t 0.4504792332268371\n"
     ]
    }
   ],
   "source": [
    "unique, counts = numpy.unique(y_train, return_counts=True)\n",
    "totalActualValues = dict(zip(unique, counts))\n",
    "countTrain = {\n",
    "      '0':0,\n",
    "      '1':0,\n",
    "      '2':0\n",
    "  }\n",
    "for i in range(len(y_train.tolist())):\n",
    "  countTrain[str(y_train.tolist()[i])]+=1\n",
    "\n",
    "print(countTrain)\n",
    "\n",
    "for key in countTrain.keys():\n",
    "  print(key,\"\\t\",countTrain[key]/len(y_train))\n",
    "\n",
    "unique, counts = numpy.unique(y_sample_test, return_counts=True)\n",
    "totalActualValues = dict(zip(unique, counts))\n",
    "\n",
    "countTest = {\n",
    "      '0':0,\n",
    "      '1':0,\n",
    "      '2':0\n",
    "  }\n",
    "for i in range(len(y_sample_test.tolist())):\n",
    "  countTest[str(y_sample_test.tolist()[i])]+=1\n",
    "\n",
    "print(countTest)\n",
    "\n",
    "\n",
    "for key in countTest.keys():\n",
    "  print(key,\"\\t\",countTest[key]/len(y_sample_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4MFOpPaGjaE"
   },
   "source": [
    "### Fine-tuning the Random Forest algorithm with the help of Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBWt4uBXP70O",
    "outputId": "90ab7993-e708-41c4-e0cb-1b150a922e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [50, 266, 483, 700, 916, 1133, 1350, 1566, 1783, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 10)]\n",
    "\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDM9NZ7IQNhj"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
    "RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(random_state=42),\n",
    "                   n_iter=100, n_jobs=-1,\n",
    "                   param_distributions={'bootstrap': [True, False],\n",
    "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
    "                                                      70, 80, 90, 100, 110,\n",
    "                                                      None],\n",
    "                                        'max_features': ['auto', 'sqrt'],\n",
    "                                        'min_samples_leaf': [1, 2, 4],\n",
    "                                        'min_samples_split': [2, 5, 10],\n",
    "                                        'n_estimators': [50, 266, 483, 700, 916,\n",
    "                                                         1133, 1350, 1566, 1783,\n",
    "                                                         2000]},\n",
    "                   random_state=42, return_train_score=True,\n",
    "                   scoring='neg_mean_absolute_error', verbose=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuJzyRfoSu0l"
   },
   "outputs": [],
   "source": [
    "# rf_random.best_params_\n",
    "\"\"\"\n",
    "{'n_estimators': 1783,\n",
    " 'min_samples_split': 10,\n",
    " 'min_samples_leaf': 1,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': 30,\n",
    " 'bootstrap': False}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKPnUF8rGp8k"
   },
   "source": [
    "### Fitting training data to the fine-tuned Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwKoaxWTMWKW",
    "outputId": "632bc332-c0ca-498d-b1d2-47995c190b20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=30, max_features='sqrt',\n",
       "                       min_samples_split=10, n_estimators=1783)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators= 1783,\n",
    " min_samples_split= 10,\n",
    " min_samples_leaf= 1,\n",
    " max_features= 'sqrt',\n",
    " max_depth= 30,\n",
    " bootstrap= False)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y88AMsPG18R"
   },
   "source": [
    "### Predicting the result for the sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VSoki6aOVOR"
   },
   "outputs": [],
   "source": [
    "y_pred_sample_test = model.predict(X_sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fS05qMhG18S"
   },
   "source": [
    "### Finding the balanced error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vi2ZjUQdQdag"
   },
   "outputs": [],
   "source": [
    "total_balanced_error_rate, errorRateOnEachClass, predictedValues = balanced_error_rate(y_pred_sample_test.tolist(),y_sample_test.values.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E33noWvaTqGS",
    "outputId": "205a0770-bf30-4a0c-c12b-efa8a9e0a052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual          Predicted 0     Predicted 1     Predicted 2     Error Rate     \n",
      "\n",
      "Class 0         126             6               95              0.44493392070484583\n",
      "Class 1         15              60              42              0.48717948717948717\n",
      "Class 2         58              8               216             0.23404255319148937\n",
      "\n",
      "\n",
      "Overall Error Rate : 0.38871865369194075\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n{:<15} {:<15} {:<15} {:<15} {:<15}\\n\".format('Actual', 'Predicted 0', 'Predicted 1', 'Predicted 2', 'Error Rate'))\n",
    " \n",
    "for keyUp, valueUP in predictedValues.items():\n",
    "  listOutput = []\n",
    "  for key, value in predictedValues[keyUp].items():\n",
    "    listOutput.append(value)\n",
    "  pred0, pred1, pred2 = listOutput[0], listOutput[1],listOutput[2]\n",
    "  print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('Class '+str(keyUp), pred0, pred1, pred2,errorRateOnEachClass[str(keyUp)]))\n",
    "\n",
    "print(\"\\n\\nOverall Error Rate : \" + str(total_balanced_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3B-fxRFG6oK"
   },
   "source": [
    "### Classification report for the rest of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ja1NTN8iFl1J",
    "outputId": "6e425ead-ce58-4883-8bde-5d47a540df14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.56      0.63      0.59       199\n",
      "     class 1       0.51      0.81      0.63        74\n",
      "     class 2       0.77      0.61      0.68       353\n",
      "\n",
      "    accuracy                           0.64       626\n",
      "   macro avg       0.61      0.69      0.63       626\n",
      "weighted avg       0.67      0.64      0.65       626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_pred_sample_test.tolist(),y_sample_test.values.tolist(), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tF3rzDjOVni"
   },
   "source": [
    "## Predicting labels for the actual test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rG9QpNbm83Xp",
    "outputId": "766e78ee-9e4d-4249-a258-fab0bba047fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=30, max_features='sqrt',\n",
       "                       min_samples_split=10, n_estimators=1783)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators= 1783,\n",
    " min_samples_split= 10,\n",
    " min_samples_leaf= 1,\n",
    " max_features= 'sqrt',\n",
    " max_depth= 30,\n",
    " bootstrap= False)\n",
    "\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTFpkNkONU3T"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test2 = df_test.drop(['index','y'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "df_min_max_scaled_test = df_test2.copy()\n",
    "for key in df_test2.keys():\n",
    "  if key!= 'y':\n",
    "    column = key\n",
    "    df_min_max_scaled_test[column] = (df_min_max_scaled_test[column] - df_min_max_scaled_test[column].min()) / (df_min_max_scaled_test[column].max() - df_min_max_scaled_test[column].min())    \n",
    "\n",
    "\n",
    "df_min_max_scaled_test = df_min_max_scaled_test.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "X_test = df_min_max_scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69XhjSu5Na0G"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UorgMc65YUoP"
   },
   "outputs": [],
   "source": [
    "df_min_max_scaled_test['y'] = y_pred\n",
    "\n",
    "df_min_max_scaled_test.to_csv(\"output_random_Forest.csv\", columns = ['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbFGZkLsIrvw"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9VEgL9HKPj2"
   },
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr0vaPJJN-gl"
   },
   "outputs": [],
   "source": [
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbbaS-4oLT7A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Xne0rkfMaX-"
   },
   "source": [
    "### Preprocessing the input to be given to the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZOjCbOTLLeE"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = np.array(X_train)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train2)\n",
    "encoded_Y = encoder.transform(y_train2)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "encoder_val= LabelEncoder()\n",
    "encoder_val.fit(y_val)\n",
    "encoded_Y_val = encoder_val.transform(y_val)\n",
    "dummy_y_val = np_utils.to_categorical(encoded_Y_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_sample_test = np.array(X_sample_test)\n",
    "encoder_sample_test = LabelEncoder()\n",
    "encoder_sample_test.fit(y_sample_test)\n",
    "encoded_Y_sample_test = encoder_sample_test.transform(y_sample_test)\n",
    "dummy_y_sample_test = np_utils.to_categorical(encoded_Y_sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk_sjxLmMgPw"
   },
   "source": [
    "### A simple neural network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMNYPcxyItFJ",
    "outputId": "e76fd989-c44d-4992-f21c-df32de073ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "226/226 [==============================] - 2s 4ms/step - loss: 0.9443 - accuracy: 0.5426 - val_loss: 0.9025 - val_accuracy: 0.5299\n",
      "Epoch 2/15\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 0.8350 - accuracy: 0.5972 - val_loss: 0.8383 - val_accuracy: 0.5976\n",
      "Epoch 3/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.8103 - accuracy: 0.6190 - val_loss: 0.8239 - val_accuracy: 0.6375\n",
      "Epoch 4/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.8048 - accuracy: 0.6203 - val_loss: 0.8375 - val_accuracy: 0.6175\n",
      "Epoch 5/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7929 - accuracy: 0.6132 - val_loss: 0.8140 - val_accuracy: 0.6295\n",
      "Epoch 6/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7924 - accuracy: 0.6226 - val_loss: 0.8752 - val_accuracy: 0.6414\n",
      "Epoch 7/15\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 0.7705 - accuracy: 0.6341 - val_loss: 0.8400 - val_accuracy: 0.6414\n",
      "Epoch 8/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7782 - accuracy: 0.6381 - val_loss: 0.8708 - val_accuracy: 0.6295\n",
      "Epoch 9/15\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 0.7577 - accuracy: 0.6461 - val_loss: 0.8379 - val_accuracy: 0.6335\n",
      "Epoch 10/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7516 - accuracy: 0.6483 - val_loss: 0.8704 - val_accuracy: 0.6375\n",
      "Epoch 11/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7539 - accuracy: 0.6519 - val_loss: 0.8577 - val_accuracy: 0.6295\n",
      "Epoch 12/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7508 - accuracy: 0.6616 - val_loss: 0.8347 - val_accuracy: 0.6215\n",
      "Epoch 13/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7538 - accuracy: 0.6532 - val_loss: 0.8612 - val_accuracy: 0.6494\n",
      "Epoch 14/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7405 - accuracy: 0.6630 - val_loss: 0.8558 - val_accuracy: 0.6056\n",
      "Epoch 15/15\n",
      "226/226 [==============================] - 1s 2ms/step - loss: 0.7312 - accuracy: 0.6643 - val_loss: 0.8678 - val_accuracy: 0.5857\n",
      "20/20 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.44      0.50       227\n",
      "           1       0.66      0.49      0.56       117\n",
      "           2       0.56      0.73      0.63       282\n",
      "\n",
      "    accuracy                           0.58       626\n",
      "   macro avg       0.60      0.55      0.56       626\n",
      "weighted avg       0.59      0.58      0.57       626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_train.shape[1],), activation='relu')) \n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train2, dummy_y, epochs=15, batch_size=10, validation_data=(X_val, dummy_y_val))\n",
    "\n",
    "\n",
    "prediction = model.predict(X_sample_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(dummy_y_sample_test.argmax(axis=1), prediction.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OTSblKWM5Eo"
   },
   "source": [
    "### Predicting the result for the sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWBUi2kuCHla"
   },
   "outputs": [],
   "source": [
    "total_balanced_error_rate, errorRateOnEachClass, predictedValues = balanced_error_rate(prediction.argmax(axis=1).tolist(),dummy_y_sample_test.argmax(axis=1).tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCGqMpClM5Eq"
   },
   "source": [
    "### Finding the balanced error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bpcvz1RBCHla",
    "outputId": "5c6867cf-3701-43c8-c29a-c8d05920376e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual          Predicted 0     Predicted 1     Predicted 2     Error Rate     \n",
      "\n",
      "Class 0         99              10              118             0.5638766519823789\n",
      "Class 1         14              57              46              0.5128205128205128\n",
      "Class 2         56              20              206             0.2695035460992908\n",
      "\n",
      "\n",
      "Overall Error Rate : 0.4487335703007274\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n{:<15} {:<15} {:<15} {:<15} {:<15}\\n\".format('Actual', 'Predicted 0', 'Predicted 1', 'Predicted 2', 'Error Rate'))\n",
    " \n",
    "# print each data item.\n",
    "for keyUp, valueUP in predictedValues.items():\n",
    "  # print(predictedValues.items())\n",
    "  listOutput = []\n",
    "  for key, value in predictedValues[keyUp].items():\n",
    "    listOutput.append(value)\n",
    "  pred0, pred1, pred2 = listOutput[0], listOutput[1],listOutput[2]\n",
    "  print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('Class '+str(keyUp), pred0, pred1, pred2,errorRateOnEachClass[str(keyUp)]))\n",
    "\n",
    "print(\"\\n\\nBalanced Error Rate : \" + str(total_balanced_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHko3oWNM5Eq"
   },
   "source": [
    "### Classification report for the rest of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84K90lSaCHlb",
    "outputId": "f8312159-7027-4cca-8f08-f78121a8fadd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.44      0.59      0.50       169\n",
      "     class 1       0.49      0.66      0.56        87\n",
      "     class 2       0.73      0.56      0.63       370\n",
      "\n",
      "    accuracy                           0.58       626\n",
      "   macro avg       0.55      0.60      0.56       626\n",
      "weighted avg       0.62      0.58      0.59       626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(prediction.argmax(axis=1).tolist(),dummy_y_sample_test.argmax(axis=1).tolist(), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcU-e8Tq1sEq"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxnIVA9mEFeV"
   },
   "outputs": [],
   "source": [
    "y=y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgaDKhnv1tq_"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "class DummyScaler:\n",
    "    def fit(self, data):\n",
    "        pass\n",
    "    def transform(self, data):\n",
    "        return data\n",
    "def create_scaler_dummy():\n",
    "    return DummyScaler()\n",
    "    \n",
    "def create_scaler_standard():\n",
    "    return StandardScaler()\n",
    "def create_scaler_minmax():\n",
    "    return MinMaxScaler()\n",
    "def crete_scaler_binarizer():\n",
    "    return Binarizer()\n",
    "    \n",
    "create_scaler = create_scaler_minmax\n",
    "\n",
    "def create_model_naive_bayes():\n",
    "    model = GaussianNB()\n",
    "    return model\n",
    "def create_model_mlpclassifier():\n",
    "    model = MLPClassifier(hidden_layer_sizes=(10,), random_state=seed)\n",
    "    return model\n",
    "def create_model_svc():\n",
    "    model = SVC(random_state=seed, probability=True)\n",
    "    return model\n",
    "    \n",
    "create_model = create_model_svc\n",
    "seed = 520\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SrkHrzNX4AM2",
    "outputId": "6d8a5fcb-abff-44cc-baff-13a00411b698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model and predict\n",
      "Model evaluation (train)\n",
      "Accuracy:\n",
      "0.6554809843400448\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.60      0.59      1056\n",
      "           1       0.92      0.42      0.57       519\n",
      "           2       0.67      0.77      0.72      1554\n",
      "\n",
      "    accuracy                           0.66      3129\n",
      "   macro avg       0.72      0.60      0.63      3129\n",
      "weighted avg       0.68      0.66      0.65      3129\n",
      "\n",
      "Confusion matrix (train)\n",
      "[[ 634    5  417]\n",
      " [ 132  216  171]\n",
      " [ 338   15 1201]]\n",
      "Cross-validation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Train the model and predict')\n",
    "scaler = create_scaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "model = create_model()\n",
    "model.fit(X, y)\n",
    "y_hat = model.predict(X)\n",
    "print('Model evaluation (train)')\n",
    "print('Accuracy:')\n",
    "print(metrics.accuracy_score(y, y_hat))\n",
    "print('Classification report:')\n",
    "print(metrics.classification_report(y, y_hat))\n",
    "print('Confusion matrix (train)')\n",
    "print (metrics.confusion_matrix(y, y_hat))\n",
    "print('Cross-validation')\n",
    "np.random.seed(seed)\n",
    "y_prob = np.zeros(y.shape)\n",
    "y_hat = np.zeros(y.shape)\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "for train, test in kfold.split(X, y):\n",
    "    scaler.fit(X[train])\n",
    "    X_train = scaler.transform(X[train])\n",
    "    X_test = scaler.transform(X[test])\n",
    "    \n",
    "    model = create_model()\n",
    "    model.fit(X_train, y[train])\n",
    "    y_prob[test] = model.predict_proba(X_test)[:, 1]\n",
    "    y_hat[test] = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWM7nMT34F_7",
    "outputId": "b407ddbf-0e1c-4991-bcdb-bb07745b2701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation (CV)\n",
      "Accuracy:\n",
      "0.62320230105465\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.57      0.55      1056\n",
      "           1       0.89      0.40      0.55       519\n",
      "           2       0.65      0.73      0.69      1554\n",
      "\n",
      "    accuracy                           0.62      3129\n",
      "   macro avg       0.69      0.57      0.60      3129\n",
      "weighted avg       0.65      0.62      0.62      3129\n",
      "\n",
      "Confusion Matrix (CV)\n",
      "[[ 607    5  444]\n",
      " [ 132  209  178]\n",
      " [ 399   21 1134]]\n",
      "Grid Search for Hyperparameters\n",
      "Optimal parameters: {'C': 1000, 'gamma': 0.01}\n",
      "Model evaluation (Optimal Hyperparameters)\n",
      "Accuracy:\n",
      "0.646964856230032\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61       213\n",
      "           1       0.91      0.39      0.54       103\n",
      "           2       0.65      0.75      0.70       310\n",
      "\n",
      "    accuracy                           0.65       626\n",
      "   macro avg       0.72      0.59      0.62       626\n",
      "weighted avg       0.67      0.65      0.64       626\n",
      "\n",
      "Confusion matrix (Optimal Hyperparameters)\n",
      "[[ 607    5  444]\n",
      " [ 132  209  178]\n",
      " [ 399   21 1134]]\n"
     ]
    }
   ],
   "source": [
    "print('Model evaluation (CV)')\n",
    "print('Accuracy:')\n",
    "print(metrics.accuracy_score(y, y_hat))\n",
    "print('Classification report:')\n",
    "print(metrics.classification_report(y, y_hat))\n",
    "print('Confusion Matrix (CV)')\n",
    "print(metrics.confusion_matrix(y, y_hat))\n",
    "\n",
    "print('Grid Search for Hyperparameters')\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, \n",
    "test_size=0.2, random_state=520)\n",
    "scaler = create_scaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Here we should use specific classifier, because of the parameters\n",
    "model = model_selection.GridSearchCV(SVC(kernel='rbf', random_state=520, \n",
    "probability=True),\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         param_grid={\n",
    "                             'C': [10**x for x in range(-3, 4)], \n",
    "                             'gamma': [10**x for x in range(-3, 4)]\n",
    "                         })\n",
    "model.fit(X_train, y_train)\n",
    "print('Optimal parameters:', model.best_params_)\n",
    "y_test_hat = model.predict(X_test)\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "print('Model evaluation (Optimal Hyperparameters)')\n",
    "print('Accuracy:')\n",
    "print(metrics.accuracy_score(y_test, y_test_hat))\n",
    "print('Classification report:')\n",
    "print(metrics.classification_report(y_test, y_test_hat))\n",
    "print('Confusion matrix (Optimal Hyperparameters)')\n",
    "print(metrics.confusion_matrix(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_bz-YgQNA_p"
   },
   "source": [
    "### Predicting the result for the sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rubcf24P8seB"
   },
   "outputs": [],
   "source": [
    "total_balanced_error_rate, errorRateOnEachClass, predictedValues = balanced_error_rate(list(y_test_hat),list(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mhBw-m6NA_q"
   },
   "source": [
    "### Finding the balanced error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLJTbpdo9NV5",
    "outputId": "89f3ef0e-bd9d-49b1-eaf0-e5265627006e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual          Predicted 0     Predicted 1     Predicted 2     Error Rate     \n",
      "\n",
      "Class 0         134             1               78              0.34801762114537443\n",
      "Class 1         18              40              45              0.5384615384615384\n",
      "Class 2         76              3               231             0.2801418439716312\n",
      "\n",
      "\n",
      "Overall Error Rate : 0.38887366785951466\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n{:<15} {:<15} {:<15} {:<15} {:<15}\\n\".format('Actual', 'Predicted 0', 'Predicted 1', 'Predicted 2', 'Error Rate'))\n",
    " \n",
    "# print each data item.\n",
    "for keyUp, valueUP in predictedValues.items():\n",
    "  # print(predictedValues.items())\n",
    "  listOutput = []\n",
    "  for key, value in predictedValues[keyUp].items():\n",
    "    listOutput.append(value)\n",
    "  pred0, pred1, pred2 = listOutput[0], listOutput[1],listOutput[2]\n",
    "  print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('Class '+str(keyUp), pred0, pred1, pred2,errorRateOnEachClass[str(keyUp)]))\n",
    "\n",
    "print(\"\\n\\nBalanced Error Rate : \" + str(total_balanced_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "971gFa8SNA_r"
   },
   "source": [
    "### Classification report for the rest of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYTpU3tn9RQa",
    "outputId": "b6731ec5-6ae7-4d61-fa75-601c13498451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.63      0.59      0.61       228\n",
      "     class 1       0.39      0.91      0.54        44\n",
      "     class 2       0.75      0.65      0.70       354\n",
      "\n",
      "    accuracy                           0.65       626\n",
      "   macro avg       0.59      0.72      0.62       626\n",
      "weighted avg       0.68      0.65      0.65       626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(list(y_test_hat),list(y_test), target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
